{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4152ae4c-91ec-4269-a48c-b42efaec683a",
   "metadata": {},
   "source": [
    "До начала Вам надо будет установить библиотеку datasets.<br>Для этого прописываем команду в anaconda_promt: **conda install -c conda-forge datasets**<br> Далее ждем загрузки и запускаем код ниже. Если выдает ошибку, попробуйте выполнить еще одну команду:<br>**conda install -c conda-forge pyarrow**<br>Должно помочь."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7371217e-2ff4-4604-bf4b-c5d620d6435c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "%matplotlib inline\n",
    "\n",
    "if tf.config.list_logical_devices(\"GPU\"):\n",
    "    print(\"CUDA is available!\")\n",
    "else:\n",
    "    print(\"CPU will be used!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c63c368-561a-48c9-877a-b8c6521f47c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['Unnamed: 0', 'text', 'sentiment'],\n",
      "        num_rows: 189891\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['Unnamed: 0', 'text', 'sentiment'],\n",
      "        num_rows: 21098\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "#Загрузим датасет\n",
    "dataset = load_dataset(\"MonoHime/ru_sentiment_dataset\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3193a2d9-ace9-4de0-a8f2-e7a95ff53fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:\n",
      "Развода на деньги нет Наблюдаюсь в Лайфклиник по беременности, развода на деньги нет, врачи не плохие, по поводу ресепшен соглашусь (путают документы). \n",
      "Label(0-Neutral, 1-Positive, 2-Negative):\n",
      "1\n",
      "Text:\n",
      "Отель выбрали потому что рядом со стадионом. Отель 4*. Номер большой. Кровать 2-спальная одна. 2 одеяла. Много подушек. Есть зона отдыха. Чайный сет. 2 бутылки по 0,5 л бесплатно каждый день. Много шкафов. Мини-бар. Утюг и гладильная доска, отдельно прибор для глажки брюк. Санузел общий большой. Ванна, лейка съемная. 2 раковины, фен. Халаты, тапочки. Расширенный пакет косметических средств, даже соль для ванны. Интернет быстрый. Так как этаж высокий был, вид из окна на город. Один недостаток в номере 919: он напротив хозлифта и с утра начинает пользоваться им персонал для обслуживания завтрака на 9 этаже. Очень слышно звон посуды и разговоры. Завтрак не включен, стоит 23 евро. Рядом с отелем есть кафе и subway.\n",
      "Label(0-Neutral, 1-Positive, 2-Negative):\n",
      "0\n",
      "Text:\n",
      "Вылечили Гноился с рождения глазик, в поликлинике назначили лечение, не помогло сходили сюда вылечили (офтальмолог Боброва - очень понравилась, все рассказала показала несколько раз как, что правильно делать), также от них приходила массажистка Ольга очень внимательная, профессионал своего дела. Спасибо. \n",
      "Label(0-Neutral, 1-Positive, 2-Negative):\n",
      "1\n",
      "Text:\n",
      "Хорошее расположение.С вокзала дошли пешком.Ночью очень тихо,хотя центр рядом.Наш номер был в башне.Размер комнаты минимальный,практически как кровать.Такая же ванная,вернее душ.Более,чем скромно.Не особо чисто.Зато дивный вид из окна и хорошее бельё.Завтрак для 4 звёзд очень скромный.Бесплатный wi-fi тормозит.Соотношение цена-качество не в пользу отеля.Цена в два раза ниже будет более адекватной.\n",
      "Label(0-Neutral, 1-Positive, 2-Negative):\n",
      "0\n",
      "Text:\n",
      "Отличное месторасположение,прекрасный вид,особенно из ресторана.Мы жили в номере с балконом и видом на Ая Софию на верхнем этаже.Номера очень чистые,небольшие,но уютные,есть все необходимое.Персонал-очень приветливый и внимательный.Завтрак-большой ассартимент холодных и горячих закусок.Останавливались уже второй раз в нем.Днем-с 4 до5 в качестве комплимента от отеля-чай со сладостями в холле.Нареканий нет.Соотношение цена-качество очень хорошее.За местоположение 5 с ++\n",
      "Label(0-Neutral, 1-Positive, 2-Negative):\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "dataset_train, dataset_val = dataset[\"train\"], dataset[\"validation\"]\n",
    "dataset_train = dataset_train.to_list()\n",
    "dataset_val = dataset_val.to_list()\n",
    "\n",
    "for x in dataset_val[:5]:\n",
    "    print(\"Text:\\n{}\\nLabel(0-Neutral, 1-Positive, 2-Negative):\\n{}\".format(x[\"text\"],x[\"sentiment\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9449ca87-3662-4066-b862-f0dab1003fee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.с.,и спросил его:  о Посланник Аллаха!Ты порицаешь что-то из слушания?  Он ответил: я не порицаю ничего из него,но передай им,чтобы они открывали свои собрания Кораном и закрывали их Кораном ...........Это дошедшие до нас мнения и тот кто находится в поисках истины,по мере изучения этого вопроса будет сталкиваться с',\n",
       " 'Роднее всех родных Попала я в ГКБ №8 еще в декабре 2002 г. Ехать в больницу очень боялась, но делать нечего, рожать надо. О ГКБ № 8 слышала много хорошего. Когда приехала в приемное отделение, мои колени тряслись от страха, но меня там встретили очень вежливо. Осмотр врача был аккуратным',\n",
       " 'Непорядочное отношение к своим работникам Работаю в сети Доктор Столетов Центр. В моем трудовом договоре прописано, что работад. оплачивает обучение по сертифик. А реально меня принуждают оплачивать половину. Жаль, но наверное придется искать новую работу!  купить китайские сотовые телефоны ',\n",
       " '). Отсутствуют нормативы, Госты и прочее, что позволило бы отличить  хорошие правильные  наркотики от  плохих неправильных . У алкоголя и табака они вырабатывались веками. Кто их будет разрабатывать? Онищенко?)) А значит отличить  хорошие  наркотики от  не хороших  будет не просто, химия не стоит',\n",
       " '           У меня машина в руках 5 лет и это первая моя машина и не могу с ней никак расстаться! Ни разу меня не подвела, заводилась после ночи на улице зимой в при -30С. В моем варианте 1500 объем, бензин,']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train = [(\" \".join(x[\"text\"].split(\" \")[:50]), x[\"sentiment\"]) for x in dataset_train][:50000]\n",
    "dataset_val = [(\" \".join(x[\"text\"].split(\" \")[:50]), x[\"sentiment\"]) for x in dataset_val][:10000]\n",
    "texts = list(list(zip(*dataset_train))[0]) + list(list(zip(*dataset_val))[0])\n",
    "texts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed9f0f84-fbfa-4f34-b103-0339260950aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 12974, 1: 23928, 2: 13098}\n"
     ]
    }
   ],
   "source": [
    "counts = {0 : 0, 1 : 0, 2 : 0,}\n",
    "for label in list(zip(*dataset_train))[1]:\n",
    "    counts[label] += 1\n",
    "\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf1e91aa-12c7-40a6-a8b3-b2b67487e983",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_texts = tf.data.Dataset.from_tensor_slices(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6339d6f8-aca0-4826-8cbd-64ea4fcaf38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import TextVectorization\n",
    "\n",
    "text_vectorization = TextVectorization(max_tokens=10000,\n",
    "                                      output_mode=\"int\",\n",
    "                                      output_sequence_length=50)\n",
    "\n",
    "text_vectorization.adapt(tf_texts, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "150e6c72-296f-4e4b-baa2-0f8911ceec8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[2889  190 1521 1222  180  305    1    1    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0]], shape=(1, 50), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "encode = text_vectorization([\"Привет, мой дорогой друг! Как ты, блаблабла, yolo?\"])\n",
    "print(encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03e899fc-0d85-4104-a256-2d800ef43f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "del texts\n",
    "del tf_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c23514d-6878-46be-9b29-8af5532e22b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"lstm_classification_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None,)]                 0         \n",
      "                                                                 \n",
      " text_vectorization (TextVec  (None, 50)               0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 50, 500)           5000000   \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 50, 500)           2002000   \n",
      "                                                                 \n",
      " norm_1 (LayerNormalization)  (None, 50, 500)          1000      \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 500)               2002000   \n",
      "                                                                 \n",
      " norm_2 (LayerNormalization)  (None, 500)              1000      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1024)              513024    \n",
      "                                                                 \n",
      " norm_dense_1 (LayerNormaliz  (None, 1024)             2048      \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " relu_1 (ReLU)               (None, 1024)              0         \n",
      "                                                                 \n",
      " drop_1 (Dropout)            (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1024)              1049600   \n",
      "                                                                 \n",
      " norm_dense_2 (LayerNormaliz  (None, 1024)             2048      \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " relu_2 (ReLU)               (None, 1024)              0         \n",
      "                                                                 \n",
      " drop_2 (Dropout)            (None, 1024)              0         \n",
      "                                                                 \n",
      " output (Dense)              (None, 3)                 3075      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,575,795\n",
      "Trainable params: 10,575,795\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.layers.Input(shape=(), dtype=\"string\", name=\"input\")\n",
    "x = text_vectorization(inputs)\n",
    "x = keras.layers.Embedding(text_vectorization.vocabulary_size(), 500, name=\"embedding\")(x)\n",
    "x = keras.layers.LSTM(500, recurrent_dropout=0.1, return_sequences=True, name=\"lstm_1\")(x)\n",
    "x = keras.layers.LayerNormalization(name=\"norm_1\")(x)\n",
    "x = keras.layers.LSTM(500, recurrent_dropout=0.1, name=\"lstm_2\")(x)\n",
    "x = keras.layers.LayerNormalization(name=\"norm_2\")(x)\n",
    "x = keras.layers.Dense(1024, name=\"dense_1\")(x)\n",
    "x = keras.layers.LayerNormalization(name=\"norm_dense_1\")(x)\n",
    "x = keras.layers.ReLU(name=\"relu_1\")(x)\n",
    "x = keras.layers.Dropout(0.25, name=\"drop_1\")(x)\n",
    "x = keras.layers.Dense(1024, name=\"dense_2\")(x)\n",
    "x = keras.layers.LayerNormalization(name=\"norm_dense_2\")(x)\n",
    "x = keras.layers.ReLU(name=\"relu_2\")(x)\n",
    "x = keras.layers.Dropout(0.25, name=\"drop_2\")(x)\n",
    "outputs = keras.layers.Dense(3, activation=\"softmax\", name=\"output\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs, name=\"lstm_classification_model\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a806039-d454-4ecf-83b0-07093e9c7ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#переведем датасеты в формат tf.data.Dataset\n",
    "tf_dataset_train = tf.data.Dataset.from_tensor_slices((list(list(zip(*dataset_train))[0]),list(list(zip(*dataset_train))[1]))).batch(50)\n",
    "tf_dataset_val = tf.data.Dataset.from_tensor_slices((list(list(zip(*dataset_val))[0]),list(list(zip(*dataset_val))[1]))).batch(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7dc6d4a-b0e3-4875-aa42-1c74b59a044a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 1.0871 - sparse_categorical_accuracy: 0.4599\n",
      "Epoch 1: val_loss improved from inf to 1.05705, saving model to fitted_text_model.tf\n",
      "INFO:tensorflow:Assets written to: fitted_text_model.tf\\assets\n",
      "1000/1000 [==============================] - 303s 299ms/step - loss: 1.0871 - sparse_categorical_accuracy: 0.4599 - val_loss: 1.0571 - val_sparse_categorical_accuracy: 0.4750\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 1.0149 - sparse_categorical_accuracy: 0.4676\n",
      "Epoch 2: val_loss improved from 1.05705 to 0.99970, saving model to fitted_text_model.tf\n",
      "INFO:tensorflow:Assets written to: fitted_text_model.tf\\assets\n",
      "1000/1000 [==============================] - 294s 294ms/step - loss: 1.0149 - sparse_categorical_accuracy: 0.4676 - val_loss: 0.9997 - val_sparse_categorical_accuracy: 0.4645\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.9667 - sparse_categorical_accuracy: 0.4787\n",
      "Epoch 3: val_loss improved from 0.99970 to 0.95992, saving model to fitted_text_model.tf\n",
      "INFO:tensorflow:Assets written to: fitted_text_model.tf\\assets\n",
      "1000/1000 [==============================] - 295s 295ms/step - loss: 0.9667 - sparse_categorical_accuracy: 0.4787 - val_loss: 0.9599 - val_sparse_categorical_accuracy: 0.5119\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.8922 - sparse_categorical_accuracy: 0.5480\n",
      "Epoch 4: val_loss improved from 0.95992 to 0.85848, saving model to fitted_text_model.tf\n",
      "INFO:tensorflow:Assets written to: fitted_text_model.tf\\assets\n",
      "1000/1000 [==============================] - 296s 296ms/step - loss: 0.8922 - sparse_categorical_accuracy: 0.5480 - val_loss: 0.8585 - val_sparse_categorical_accuracy: 0.5721\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.7733 - sparse_categorical_accuracy: 0.6188\n",
      "Epoch 5: val_loss improved from 0.85848 to 0.80791, saving model to fitted_text_model.tf\n",
      "INFO:tensorflow:Assets written to: fitted_text_model.tf\\assets\n",
      "1000/1000 [==============================] - 297s 297ms/step - loss: 0.7733 - sparse_categorical_accuracy: 0.6188 - val_loss: 0.8079 - val_sparse_categorical_accuracy: 0.6154\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.6790 - sparse_categorical_accuracy: 0.6760\n",
      "Epoch 6: val_loss improved from 0.80791 to 0.74438, saving model to fitted_text_model.tf\n",
      "INFO:tensorflow:Assets written to: fitted_text_model.tf\\assets\n",
      "1000/1000 [==============================] - 305s 305ms/step - loss: 0.6790 - sparse_categorical_accuracy: 0.6760 - val_loss: 0.7444 - val_sparse_categorical_accuracy: 0.6357\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.6039 - sparse_categorical_accuracy: 0.7212\n",
      "Epoch 7: val_loss did not improve from 0.74438\n",
      "1000/1000 [==============================] - 297s 297ms/step - loss: 0.6039 - sparse_categorical_accuracy: 0.7212 - val_loss: 0.7682 - val_sparse_categorical_accuracy: 0.6502\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.5445 - sparse_categorical_accuracy: 0.7568\n",
      "Epoch 8: val_loss did not improve from 0.74438\n",
      "1000/1000 [==============================] - 298s 298ms/step - loss: 0.5445 - sparse_categorical_accuracy: 0.7568 - val_loss: 0.7992 - val_sparse_categorical_accuracy: 0.6464\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.4963 - sparse_categorical_accuracy: 0.7816\n",
      "Epoch 9: val_loss did not improve from 0.74438\n",
      "1000/1000 [==============================] - 295s 295ms/step - loss: 0.4963 - sparse_categorical_accuracy: 0.7816 - val_loss: 0.8365 - val_sparse_categorical_accuracy: 0.6468\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.4573 - sparse_categorical_accuracy: 0.8045\n",
      "Epoch 10: val_loss did not improve from 0.74438\n",
      "1000/1000 [==============================] - 291s 291ms/step - loss: 0.4573 - sparse_categorical_accuracy: 0.8045 - val_loss: 0.9164 - val_sparse_categorical_accuracy: 0.6420\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.4147 - sparse_categorical_accuracy: 0.8256\n",
      "Epoch 11: val_loss did not improve from 0.74438\n",
      "1000/1000 [==============================] - 299s 299ms/step - loss: 0.4147 - sparse_categorical_accuracy: 0.8256 - val_loss: 0.9506 - val_sparse_categorical_accuracy: 0.6493\n",
      "Epoch 11: early stopping\n"
     ]
    }
   ],
   "source": [
    "#обучение будет довольно долгим на средних компьютерах, так что можно просто перейти на следующий блок,\n",
    "#скачав модель с гитхаба и переместив ее в папку с этим notebook'ом\n",
    "lr_sched= tf.keras.optimizers.schedules.ExponentialDecay(0.001, 1000, 0.96)\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr_sched),\n",
    "             loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "             metrics=keras.metrics.SparseCategoricalAccuracy())\n",
    "\n",
    "history = model.fit(tf_dataset_train, validation_data=tf_dataset_val, epochs=100, shuffle=True,\n",
    "                    callbacks=[keras.callbacks.EarlyStopping(patience=5, verbose=1), \n",
    "                               keras.callbacks.ModelCheckpoint(\"fitted_text_model.tf\", save_best_only=True, verbose=1)], \n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29e424a6-55d8-4425-a9c9-e51ebeff2eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model(\"fitted_text_model.tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "58fa8f72-1c4c-4c45-99af-e6b15c68f640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 123ms/step\n",
      "[[0.4103907  0.5462118  0.04339749]]\n",
      "Оу, ты явно написал что-то милое. Надеюсь, что это не ошибка системы...\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "promt = \"\"\"На рассвете первые лучи солнца осторожно касались земли, пробуждая всё вокруг. \n",
    "           Лёгкий ветерок переносил ароматы цветов, шёпот листвы и обещания нового дня. \n",
    "           Мир замирал в тишине, где каждая деталь обретала смысл. \n",
    "           В эти мгновения казалось, что сама природа напоминала нам ценить её простоту и красоту.\n",
    "        \"\"\"\n",
    "promt = np.array([promt], dtype=np.str_).reshape(1,-1)\n",
    "\n",
    "prediction = model.predict(promt)\n",
    "print(prediction)\n",
    "prediction = prediction.argmax().item()\n",
    "\n",
    "if prediction == 0:\n",
    "    print(\"Ну, ок! Ноль эмоций -_-\")\n",
    "elif prediction == 1:\n",
    "    print(\"Оу, ты явно написал что-то милое. Надеюсь, что это не ошибка системы...\")\n",
    "else:\n",
    "    display(Image(url=\"https://avatars.mds.yandex.net/i?id=ed7efab13ea582cfb7ea67af71a64662b54b70f9-11005657-images-thumbs&n=13\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f999b1f9-7cf8-4ce7-9f37-02770c1a54c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#примеры положительного, нейтрального и негативного текстов\n",
    "promt = \"\"\"На рассвете первые лучи солнца осторожно касались земли, пробуждая всё вокруг. \n",
    "           Лёгкий ветерок переносил ароматы цветов, шёпот листвы и обещания нового дня. \n",
    "           Мир замирал в тишине, где каждая деталь обретала смысл. \n",
    "           В эти мгновения казалось, что сама природа напоминала нам ценить её простоту и красоту.\n",
    "        \"\"\"\n",
    "promt = \"\"\"Современные технологии оказывают значительное влияние на повседневную жизнь человека. \n",
    "           Сфера применения инноваций охватывает множество областей: от медицины и образования до промышленности и развлечений. \n",
    "           Развитие искусственного интеллекта и автоматизации позволяет решать сложные задачи быстрее и эффективнее, \n",
    "           однако требует продуманного подхода для минимизации возможных рисков и этических вопросов.\n",
    "        \"\"\"\n",
    "promt = \"\"\"Слушай, ты ведь можешь лучше. \n",
    "           Ты берёшься за интересные вещи, но почему не доводишь их до конца? \n",
    "           Перестань откладывать на потом. Это только твоя ответственность. \n",
    "           Если не начнёшь действовать сейчас, кто будет за это отвечать? \n",
    "           Ты знаешь, что у тебя есть потенциал — так используй его!\n",
    "        \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
